{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57112383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca36eb5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Takes the text as input and save it  in the file specified in the implementation\"\"\"\n",
    "def append_to_file(text):\n",
    "    with open(\"outputs_optimized_layer_9_label_4.txt\", \"a\") as file:\n",
    "        # Write content to the file\n",
    "        file.write(f\"{text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b6892a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_except_pca(label, train, valid, append_to_file = False, testSet = False):\n",
    "\n",
    "    dropping_labels = [\"label_1\", \"label_2\",\"label_3\", \"label_4\"]\n",
    "    # other labels should drop\n",
    "    dropping_labels.remove(label)\n",
    "    print(f\"Running for {label} \")\n",
    "\n",
    "    train.drop(dropping_labels, axis=1, inplace=True)\n",
    "    \n",
    "    if not testSet:\n",
    "        valid.drop(dropping_labels, axis=1, inplace=True)\n",
    "\n",
    "    if(len(train.columns[train.isnull().any()])>0):\n",
    "        print(f\"{label} has missing values in train set\")\n",
    "        train.dropna(inplace=True)\n",
    "\n",
    "    if(len(valid.columns[valid.isnull().any()])>0):\n",
    "        print(f\"{label} has missing values in valid set\")\n",
    "        valid.dropna(inplace=True)\n",
    "\n",
    "    # splitting features and the label\n",
    "    x_train = train.drop([label], axis=1)\n",
    "    y_train = train[label]\n",
    "    \n",
    "    if not testSet:\n",
    "        x_valid = valid.drop([label], axis=1)\n",
    "        y_valid = valid[label]\n",
    "    else:\n",
    "        x_valid = valid.drop(['ID'], axis=1)\n",
    "\n",
    "    # print nessasary stuff\n",
    "    print(f\"initial train set shape={x_train.shape}\")\n",
    "    if append_to_file:\n",
    "        append_to_file(f\"initial train set shape={x_train.shape}\")\n",
    "\n",
    "    # initiate over sampling strategy\n",
    "    smote = SMOTE(sampling_strategy='auto', random_state=42)  # You can adjust the sampling strategy\n",
    "\n",
    "    # Fit and transform the dataset\n",
    "    # rx_train, ry_train = smote.fit_resample(x_train, y_train)\n",
    "    rx_train, ry_train = x_train, y_train\n",
    "\n",
    "    # print after oversampling stuff\n",
    "    print(f\"Resampled train set shape={rx_train.shape}\")\n",
    "    if append_to_file:\n",
    "        append_to_file(f\"Resampled train set shape={rx_train.shape}\")\n",
    "\n",
    "    # init the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "\n",
    "    # fit the scaler\n",
    "    sx_train = pd.DataFrame(scaler.fit_transform(rx_train), columns=rx_train.columns)\n",
    "    sx_valid = pd.DataFrame(scaler.transform(x_valid), columns=x_valid.columns)\n",
    "    \n",
    "    if not testSet:\n",
    "        return sx_train, sx_valid, ry_train, y_valid\n",
    "    else:\n",
    "        return sx_train, sx_valid, ry_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2aea0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_pca(sx_train, sx_valid, n_comp = None):\n",
    "    if n_comp is not None:\n",
    "        pca = PCA(n_components= n_comp)\n",
    "\n",
    "        psx_train = pca.fit_transform(sx_train)\n",
    "        psx_valid = pca.transform(sx_valid)\n",
    "        \n",
    "        new_len = len(psx_train[0])\n",
    "        \n",
    "        psx_train = pd.DataFrame(psx_train, columns=[f\"new_label{i}\" for i in range(1, len(psx_train[0])+1)])\n",
    "        psx_valid = pd.DataFrame(psx_valid, columns=[f\"new_label{i}\" for i in range(1, len(psx_valid[0])+1)])\n",
    "    else:\n",
    "        psx_train = sx_train\n",
    "        psx_valid = sx_valid\n",
    "    return psx_train, psx_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "435b1770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for label_1 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sx_train, sx_valid \u001b[38;5;241m=\u001b[39m \u001b[43mget_preprocessed_except_pca\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mappend_to_file\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n_comp \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m0.97\u001b[39m, \u001b[38;5;241m0.98\u001b[39m, \u001b[38;5;241m0.99\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning for n_component\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_comp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 7\u001b[0m, in \u001b[0;36mget_preprocessed_except_pca\u001b[1;34m(label, append_to_file)\u001b[0m\n\u001b[0;32m      4\u001b[0m dropping_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_3\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel_4\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m train \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./train.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m valid \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./valid.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m train\u001b[38;5;241m.\u001b[39mdrop(dropping_labels, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    610\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 611\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1771\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m     (\n\u001b[0;32m   1775\u001b[0m         index,\n\u001b[0;32m   1776\u001b[0m         columns,\n\u001b[0;32m   1777\u001b[0m         col_dict,\n\u001b[1;32m-> 1778\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[0;32m   1779\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[0;32m   1780\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:232\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    230\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    231\u001b[0m     \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43m_concatenate_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    235\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:402\u001b[0m, in \u001b[0;36m_concatenate_chunks\u001b[1;34m(chunks)\u001b[0m\n\u001b[0;32m    390\u001b[0m             result[name] \u001b[38;5;241m=\u001b[39m array_type\u001b[38;5;241m.\u001b[39m_concat_same_type(\n\u001b[0;32m    391\u001b[0m                 arrs  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    392\u001b[0m             )\n\u001b[0;32m    393\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    394\u001b[0m             \u001b[38;5;66;03m# error: Argument 1 to \"concatenate\" has incompatible\u001b[39;00m\n\u001b[0;32m    395\u001b[0m             \u001b[38;5;66;03m# type \"List[Union[ExtensionArray, ndarray[Any, Any]]]\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m             \u001b[38;5;66;03m# , Sequence[Sequence[Sequence[Sequence[\u001b[39;00m\n\u001b[0;32m    401\u001b[0m             \u001b[38;5;66;03m# _SupportsArray[dtype[Any]]]]]]]\"\u001b[39;00m\n\u001b[1;32m--> 402\u001b[0m             result[name] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m warning_columns:\n\u001b[0;32m    405\u001b[0m     warning_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(warning_columns)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#\n",
    "# Run only when need know parameters\n",
    "#\n",
    "train = pd.read_csv(\"./train.csv\")\n",
    "valid = pd.read_csv(\"./valid.csv\")\n",
    "sx_train, sx_valid = get_preprocessed_except_pca(label=\"label_1\",train=train, valid=valid, append_to_file=True)\n",
    "for n_comp in [0.97, 0.98, 0.99, None]:\n",
    "    print(f\"Running for n_component{n_comp} \")\n",
    "    psx_train, psx_valid = do_pca(sx_train, sx_valid, n_comp=n_comp)\n",
    "    print(f\"No of new coums is {len(psx_train.columns)}.\")\n",
    "    append_to_file(f\"No of new coums is {len(psx_train.columns)}.\")\n",
    "    # Create an instance of MyModel\n",
    "#         init_model = SVC()\n",
    "\n",
    "#         # Fit the model to the training data\n",
    "#         init_model.fit(x_train, y_train)\n",
    "\n",
    "#         # Make predictions on the test data\n",
    "#         y_pred = init_model.predict(x_valid)\n",
    "\n",
    "#         # Print the accuracy of the model\n",
    "#         accuracy = (y_pred == y_valid).mean()\n",
    "#         print(f\"Accuracy for {label} with n_comp {n_comp}: {accuracy}\")\n",
    "#         append_to_file(f\"Initial accuracy for {label} with n_comp {n_comp}: {accuracy}\")\n",
    "\n",
    "    # Example of using RandomizedSearchCV to tune hyperparameters\n",
    "    param_dist = {\n",
    "        'C': [i for i in range(90,105)],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': uniform(0.0009, 0.01),\n",
    "        \"class_weight\": [\"balanced\"]\n",
    "    }\n",
    "    \n",
    "    svc = SVC()\n",
    "\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=svc,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=15,  # Number of random combinations to try\n",
    "        cv=5,  # Number of cross-validation folds\n",
    "        verbose=2,\n",
    "        random_state=42,  # Set a random seed for reproducibility\n",
    "        n_jobs=-1  # Use all available CPU cores for parallel computation\n",
    "    )\n",
    "    \n",
    "    full_x = pd.concat([psx_train,psx_valid], axis = 0)\n",
    "    full_y = pd.concat([ry_train, y_valid], axis = 0)\n",
    "    \n",
    "    random_search.fit(full_x, full_y)\n",
    "\n",
    "    print(f\"Best hyperparameters found by RandomizedSearchCV for label {label} with n_comp {n_comp}:\")\n",
    "    print(random_search.best_params_)\n",
    "    append_to_file(f\"Best params for {label} with n_comp {n_comp}: {random_search.best_params_}\")\n",
    "\n",
    "    print(f\"Best Score: for label {label} with n_comp {n_comp}\", random_search.best_score_)\n",
    "    append_to_file(f\"Best score for {label} with n_comp {n_comp}: {random_search.best_score_}\")\n",
    "\n",
    "    # Perform cross-validation to evaluate the model with the best hyperparameters\n",
    "#     cross_val_scores = cross_val_score(random_search, X, y, cv=5, n_jobs=-1)\n",
    "\n",
    "    # Print cross-validation scores\n",
    "#     print(\"Cross-Validation Scores:\", cross_val_scores)\n",
    "#     append_to_file(f\"Cross-Validation Scores for {label} : {cross_val_scores} \\n\")\n",
    "#     print(\"Mean CV Score:\", np.mean(cross_val_scores))\n",
    "#     append_to_file(f\"Mean CV Score for {label} : {np.mean(cross_val_scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0bf51809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(label, n_comp = 0.98, kernal=\"rbf\", gamma=0.001, C=100, class_weight= \"balanced\"):\n",
    "    \n",
    "    train=pd.read_csv(\"./train.csv\")\n",
    "    valid=pd.read_csv(\"./valid.csv\")\n",
    "    \n",
    "    sx_train, sx_valid, ry_train, y_valid = get_preprocessed_except_pca(label=label, train=train, valid=valid, append_to_file=False)\n",
    "    psx_train, psx_valid = do_pca(sx_train, sx_valid, n_comp=n_comp)\n",
    "    \n",
    "    full_x = pd.concat([psx_train,psx_valid], axis = 0)\n",
    "    full_y = pd.concat([ry_train, y_valid], axis = 0)\n",
    "    \n",
    "    # Convert the DataFrames to NumPy arrays\n",
    "    X = full_x.to_numpy()\n",
    "    y = full_y.to_numpy().ravel()  # Flatten the labels to a 1D array\n",
    "\n",
    "    # Specify the number of folds for cross-validation (k=5)\n",
    "    k = 5\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    # Initialize an SVM classifier (you can specify the kernel and other hyperparameters)\n",
    "    classifier = SVC(kernel=kernal, gamma=gamma, C=C, class_weight=class_weight)  # You can change the kernel type as needed\n",
    "\n",
    "    # Perform k-fold cross-validation\n",
    "    cross_val_scores = cross_val_score(classifier, X, y, cv=kf)\n",
    "\n",
    "    # Print the cross-validation scores\n",
    "    print(\"Cross-validation scores:\", cross_val_scores)\n",
    "\n",
    "    # Calculate and print the mean and standard deviation of the cross-validation scores\n",
    "    mean_score = np.mean(cross_val_scores)\n",
    "    std_deviation = np.std(cross_val_scores)\n",
    "    print(\"Mean accuracy:\", mean_score)\n",
    "    print(\"Standard deviation of accuracy:\", std_deviation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9e6a3643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for label_1 \n",
      "initial train set shape=(28520, 768)\n",
      "Resampled train set shape=(28520, 768)\n",
      "Cross-validation scores: [0.96276051 0.95439016 0.95985651 0.95678169 0.9554151 ]\n",
      "Mean accuracy: 0.9578407926204304\n",
      "Standard deviation of accuracy: 0.003071782274409166\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(label=\"label_1\", n_comp = 0.99, kernal=\"rbf\", gamma=0.001, C=100, class_weight= \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ff4923ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_test(label, n_comp = 0.98, kernal=\"rbf\", gamma=0.001, C=100, class_weight= \"balanced\"):\n",
    "    train = pd.concat([pd.read_csv(\"./train.csv\"),pd.read_csv(\"./valid.csv\")], axis=0)\n",
    "    test = pd.read_csv(\"./test.csv\")\n",
    "    \n",
    "    sx_train, sx_test, ry_train = get_preprocessed_except_pca(label=label,train=train, valid=test, append_to_file=False, testSet=True)\n",
    "    psx_train, psx_test = do_pca(sx_train, sx_test, n_comp=n_comp)\n",
    "    \n",
    "    classifier = SVC(kernel=kernal, gamma=gamma, C=C, class_weight=class_weight)\n",
    "    \n",
    "    classifier.fit(psx_train, ry_train)\n",
    "    \n",
    "    result = classifier.predict(psx_test)\n",
    "    \n",
    "    res_df = pd.DataFrame(result, columns=[label])\n",
    "#     res_df.to_csv(f\"./190438H_{label}.csv\")\n",
    "    return res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d4aeef8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for label_1 \n",
      "initial train set shape=(29270, 768)\n",
      "Resampled train set shape=(29270, 768)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>739</th>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>740</th>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>744 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label_1\n",
       "0         26\n",
       "1         18\n",
       "2         16\n",
       "3          7\n",
       "4         58\n",
       "..       ...\n",
       "739       40\n",
       "740       35\n",
       "741       54\n",
       "742       38\n",
       "743       51\n",
       "\n",
       "[744 rows x 1 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_predict_test(label=\"label_1\", n_comp = 0.98, kernal=\"rbf\", gamma=0.001, C=100, class_weight= \"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "288f928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    {\"label\": \"label_1\",\"n_comp\": None, \"kernal\":\"rbf\", \"gamma\":0.001464, \"C\":95, \"class_weight\":\"balanced\" },\n",
    "    {\"label\": \"label_2\",\"n_comp\": None, \"kernal\":\"rbf\", \"gamma\":0.001464, \"C\":95, \"class_weight\":\"balanced\" },\n",
    "    {\"label\": \"label_3\",\"n_comp\": None, \"kernal\":\"rbf\", \"gamma\":0.001464, \"C\":95, \"class_weight\":\"balanced\" },\n",
    "    {\"label\": \"label_4\",\"n_comp\": None, \"kernal\":\"rbf\", \"gamma\":0.001464, \"C\":95, \"class_weight\":\"balanced\" },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fe470651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for label_1 \n",
      "initial train set shape=(29270, 768)\n",
      "Resampled train set shape=(29270, 768)\n",
      "Running for label_2 \n",
      "label_2 has missing values in train set\n",
      "initial train set shape=(28776, 768)\n",
      "Resampled train set shape=(28776, 768)\n",
      "Running for label_3 \n",
      "initial train set shape=(29270, 768)\n",
      "Resampled train set shape=(29270, 768)\n",
      "Running for label_4 \n",
      "initial train set shape=(29270, 768)\n",
      "Resampled train set shape=(29270, 768)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for row in config:\n",
    "    dfs.append(train_and_predict_test(label=row[\"label\"], n_comp=row[\"n_comp\"], kernal=row[\"kernal\"], gamma=row[\"gamma\"], C=row[\"C\"], class_weight=row[\"class_weight\"]))\n",
    "final = pd.concat(dfs, axis=1)\n",
    "final[\"ID\"] = [i for i in range(1, final.shape[0]+1)]\n",
    "final.to_csv(\"190438H_layer_9_att_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e5271454",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = [\n",
    "    {\"label\": \"label_1\",\"n_comp\": 0.99, \"kernal\":\"rbf\", \"gamma\":0.001, \"C\":100, \"class_weight\":\"balanced\" },\n",
    "    {\"label\": \"label_2\",\"n_comp\": 0.98, \"kernal\":\"rbf\", \"gamma\":0.001, \"C\":100, \"class_weight\":\"balanced\" },\n",
    "    {\"label\": \"label_3\",\"n_comp\": 0.97, \"kernal\":\"rbf\", \"gamma\":0.001, \"C\":100, \"class_weight\":\"balanced\" },\n",
    "    {\"label\": \"label_4\",\"n_comp\": 0.98, \"kernal\":\"rbf\", \"gamma\":0.001, \"C\":100, \"class_weight\":\"balanced\" },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1bad36da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for label_1 \n",
      "initial train set shape=(29270, 768)\n",
      "Resampled train set shape=(29270, 768)\n",
      "Running for label_2 \n",
      "label_2 has missing values in train set\n",
      "initial train set shape=(28776, 768)\n",
      "Resampled train set shape=(28776, 768)\n",
      "Running for label_3 \n",
      "initial train set shape=(29270, 768)\n",
      "Resampled train set shape=(29270, 768)\n",
      "Running for label_4 \n",
      "initial train set shape=(29270, 768)\n",
      "Resampled train set shape=(29270, 768)\n"
     ]
    }
   ],
   "source": [
    "dfs = []\n",
    "for row in config:\n",
    "    dfs.append(train_and_predict_test(label=row[\"label\"], n_comp=row[\"n_comp\"], kernal=row[\"kernal\"], gamma=row[\"gamma\"], C=row[\"C\"], class_weight=row[\"class_weight\"]))\n",
    "final = pd.concat(dfs, axis=1)\n",
    "final[\"ID\"] = [i for i in range(1, final.shape[0]+1)]\n",
    "final.to_csv(\"190438H_layer_9_att_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a9341",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
